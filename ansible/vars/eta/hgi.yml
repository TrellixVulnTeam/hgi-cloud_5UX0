---
base_user_name: hgi
base_user_uid: 10000
base_user_home: "/opt/sanger.ac.uk/{{ base_user_name }}"
base_user_groups:
  - name: informatics
    gid: 11000
base_user_shell: /bin/bash

base_group_name: "{{ base_user_name }}"
base_group_gid: "{{ base_user_uid }}"

base_source_dir: "{{ base_user_home }}/src"
base_download_dir: "{{ base_user_home }}/download"
base_bin_dir: "{{ base_user_home }}/bin"

base_dir_owner: "{{ base_user_name }}"
base_dir_group: "{{ base_group_name }}"
base_dir_mode: 0775

base_install_owner: "{{ base_user_name }}"
base_install_group: "{{ base_group_name }}"
base_install_dir: "{{ base_user_home }}"

anaconda_source_dir: "{{ base_source_dir }}"
anaconda_download_dir: "{{ base_download_dir }}"
anaconda_install_dir: "{{ base_install_dir }}"

anaconda_install_owner: "{{ base_install_owner }}"
anaconda_install_group: "{{ base_install_group }}"
anaconda_install_mode: "u+rw,go+r"

hadoop_user_name: "{{ base_user_name }}"
hadoop_group_name: "{{ hadoop_user_name }}"

hadoop_download_dir: "{{ base_download_dir }}"
hadoop_install_dir: "{{ base_install_dir }}"

hadoop_home: "{{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}"

hadoop_install_owner: "{{ hadoop_user_name }}"
hadoop_install_group: "{{ hadoop_group_name }}"
hadoop_install_mode: "ug+rw,o+r"

hadoop_pid_dir: "{{ hadoop_home }}/run"
hadoop_log_dir: "{{ hadoop_home }}/log"
hadoop_tmp_dir: "{{ hadoop_home }}/tmp"

spark_user_name: "{{ base_user_name }}"
spark_group_name: "{{ base_group_name }}"

spark_source_dir: "{{ base_source_dir }}"
spark_download_dir: "{{ base_download_dir }}"
spark_install_dir: "{{ base_user_home }}"

spark_install_owner: "{{ base_install_owner }}"
spark_install_group: "{{ base_install_group }}"
spark_install_mode: "ug+rw,o+r"

spark_pid_dir: "{{ spark_home }}/run"
spark_log_dir: "{{ spark_home }}/log"
spark_temp_dir: "{{ spark_home }}/temp"
spark_work_dir: "{{ spark_home }}/work"
spark_conf_dir: "{{ spark_home }}/conf"

spark_hadoop_profile: hadoop-2.7

spark_distribution_name: "hgi-hadoop{{ hadoop_version }}"
spark_home: "{{ spark_install_dir }}/spark-{{ spark_version }}-bin-{{ spark_distribution_name }}"

spark_local_owner: "{{ spark_user_name }}"
spark_local_group: "{{ spark_group_name }}"
spark_local_mode: "ug+rw,o+r,+t"
spark_local_dirs:
  - "{{ spark_temp_dir }}"

spark_master_port: 7077

spark_env_extras: {}

spark_extra_classpath:
  - "{{ hail_home }}/build/libs/hail-all-spark.jar"
  - "{{ hadoop_home }}/share/hadoop/tools/lib/aws-java-sdk-core-1.7.4.jar"
  - "{{ hadoop_home }}/share/hadoop/tools/lib/aws-java-sdk-kms-1.7.4.jar"
  - "{{ hadoop_home }}/share/hadoop/tools/lib/aws-java-sdk-s3-1.7.4.jar"
  - "{{ hadoop_home }}/share/hadoop/tools/lib/hadoop-aws-7.3.2.jar"

# spark_executor_extra_classpath: "{{ spark_extra_classpath }}"
# spark_driver_extra_classpath: "{{ spark_extra_classpath }}"
# spark_jars: "{{ spark_extra_classpath }}"

spark_defaults_extras:
  # SparkContext Configuration
  spark.master: "spark://spark-master:{{ spark_master_port }}"
  spark.sql.files.maxPartitionBytes: "42949672960"
  spark.sql.files.openCostInBytes: "42949672960"

  spark.driver.memory: 60g
  spark.driver.maxResultSize: 0
  spark.driver.extraClassPath: "{{ spark_extra_classpath | join(':') }}"

  spark.executor.memory: 60g
  spark.executor.instances: 1
  spark.executor.extraClassPath: "{{ spark_extra_classpath | join(':') }}"

  spark.jars: "{{ spark_extra_classpath | join(':') }}"

  # spark.ui.reverseProxy: true
  # spark.ui.reverseProxyUrl: "https://{# spark_master_external_hostname #}.{# spark_master_external_domain #}/{# spark_master_external_path #}"

  spark.serializer: "org.apache.spark.serializer.KryoSerializer"
  spark.kryo.registrator: "is.hail.kryo.HailKryoRegistrator"

  # Hadoop Configuration for S3
  spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
  spark.hadoop.fs.s3a.endpoint: cog.sanger.ac.uk
  spark.hadoop.fs.s3a.connection.maximum: 100
  # spark.hadoop.fs.s3a.bucket.<bucket_name>.aws.credentials.provider:  org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider
  spark.ui.reverseProxy: true
  spark.ui.reverseProxyUrl: "http://{{ extra_user_data['spark_master_external_address'] }}/spark"


# Logger levels set to the official defaults
# Ref: https://github.com/apache/spark/blob/master/conf/log4j.properties.template
spark_log4j_extras:
  - name: log4j.logger.org.apache.spark.repl.Main
    value: WARN
  - name: log4j.logger.org.spark_project.jetty
    value: WARN
  - name: log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle
    value: ERROR
  - name: log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper
    value: INFO
  - name: log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
    value: INFO
  - name: log4j.logger.org.apache.parquet
    value: ERROR
  - name: log4j.logger.parquet
    value: ERROR
  - name: log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler
    value: FATAL
  - name: log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry
    value: ERROR

spark_sysctl_params:
  - name: kernel.msgmnb
    value: 65536
  - name: kernel.msgmax
    value: 65536
  - name: net.ipv4.tcp_max_tw_buckets
    value: 4000000
  - name: net.core.rmem_max
    value: 67108864
  - name: net.core.wmem_max
    value: 67108864
  - name: net.core.optmem_max
    value: 67108864
  - name: net.ipv4.tcp_rmem
    value: 4096 16777216 67108864
  - name: net.ipv4.tcp_wmem
    value: 4096 16777216 67108864
  - name: net.ipv4.tcp_mem
    value: 67108864 67108864 67108864
  - name: net.core.somaxconn
    value: 640000
  - name: net.core.netdev_max_backlog
    value: 250000
  - name: net.ipv4.tcp_max_syn_backlog
    value: 200000
  - name: net.ipv4.tcp_dsack
    value: 0
  - name: net.ipv4.tcp_sack
    value: 0
  - name: net.ipv4.tcp_window_scaling
    value: 1
  - name: net.ipv4.ip_local_port_range
    value: 8196 65535
  - name: net.ipv4.ip_local_reserved_ports
    # TODO: ensure spark_master_port spark_master_backend_port are on this list
    value: 7077,7337,8000-8088,8141,8188,8440-8485,8651-8670,8788,8983,9083,9898,10000-10033,10200,11000,13562,15000,19888,45454,50010,50020,50030,50060,50070,50075,50090,50091,50470,50475,50100,50105,50111,60010-60030
  - name: net.ipv4.tcp_retries2
    value: 10
  - name: net.ipv4.tcp_rfc1337
    value: 1
  - name: net.ipv4.tcp_fin_timeout
    value: 5
  - name: net.ipv4.tcp_keepalive_intvl
    value: 15
  - name: net.ipv4.tcp_keepalive_probes
    value: 5
  - name: vm.min_free_kbytes
    value: 204800
  - name: vm.page-cluster
    value: 20
  - name: vm.swappiness
    value: 10
  - name: fs.file-max
    value: 5049800

hail_install_dir: "{{ base_install_dir }}"
hail_install_owner: "{{ base_install_owner }}"
hail_install_group: "{{ base_install_group }}"
hail_install_mode: "ug+rw,o+r"

hail_home: "{{ hail_install_dir }}/hail/hail"
hail_shared_dir: "{{ spark_home }}/local"
