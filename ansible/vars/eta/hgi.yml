---
# Do not change the values in this file, unless you know what are you doing.
# Aa a rule of thumb, never change a value unless you are sure of the implications.

# Basic Configuration.

# The following values are safe for the user to copy in their own file and
# customize. Being safe, doesn't mean the result will be a working cluster: for
# instance, if you change the version of spark, you also want to be sure there
# is a spark binary distribution with the same version, available to be
# downloaded and installed.

anaconda_distribution: 2019.03-Linux-x86_64
anaconda_python_version: "3.7"
aws_java_sdk_version: "1.7.4"
hadoop_version: "2.7.1"
java_jdk_version: "8"
spark_hadoop_profile: "2.7"
spark_version: "2.4.3"
hail_version: "0.2.19"

# A list of extra jars that the users might want to install. It is possible to
# install them with a different name.
hail_extra_jars:
  - filename: "aws-java-sdk-{{ aws_java_sdk_version }}.jar"
    url: "http://search.maven.org/remotecontent?filepath=com/amazonaws/aws-java-sdk/{{ aws_java_sdk_version }}/aws-java-sdk-{{ aws_java_sdk_version }}.jar"
  - filename: "hadoop-aws-{{ hadoop_version }}.jar"
    url: "https://search.maven.org/remotecontent?filepath=org/apache/hadoop/hadoop-aws/{{ hadoop_version }}/hadoop-aws-{{ hadoop_version }}.jar"

# A list of extra Debian/Ubuntu packages that the users might want to install.
hail_extra_apt: []

# A list of extra python modules (in the anaconda distribution) that the user
# might want to install.
hail_extra_pip: []

# Advanced Configuration

base_user_name: hgi
base_user_uid: 10000
base_user_home: "/opt/sanger.ac.uk/{{ base_user_name }}"
base_user_groups:
  - name: informatics
    gid: 11000
base_user_shell: /bin/bash

base_group_name: "{{ base_user_name }}"
base_group_gid: "{{ base_user_uid }}"

base_source_dir: "{{ base_user_home }}/src"
base_download_dir: "{{ base_user_home }}/download"
base_bin_dir: "{{ base_user_home }}/bin"

base_install_owner: "{{ base_user_name }}"
base_install_group: "{{ base_group_name }}"
base_install_dir: "{{ base_user_home }}"
base_install_mode: "ug+rw,o-w"

anaconda_source_dir: "{{ base_source_dir }}"
anaconda_download_dir: "{{ base_download_dir }}"
anaconda_install_dir: "{{ base_install_dir }}"

anaconda_install_owner: "{{ base_install_owner }}"
anaconda_install_group: "{{ base_install_group }}"
anaconda_install_mode: "{{ base_install_mode }}"

hadoop_user_name: "{{ base_user_name }}"
hadoop_group_name: "{{ hadoop_user_name }}"

hadoop_download_dir: "{{ base_download_dir }}"
hadoop_install_dir: "{{ base_install_dir }}"

hadoop_home: "{{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}"

hadoop_install_owner: "{{ hadoop_user_name }}"
hadoop_install_group: "{{ hadoop_group_name }}"
hadoop_install_mode: "{{ base_install_mode }}"

hadoop_pid_dir: "{{ hadoop_home }}/run"
hadoop_log_dir: "{{ hadoop_home }}/log"
hadoop_tmp_dir: "{{ hadoop_home }}/tmp"

spark_user_name: "{{ base_user_name }}"
spark_group_name: "{{ base_group_name }}"

spark_source_dir: "{{ base_source_dir }}"
spark_download_dir: "{{ base_download_dir }}"
spark_install_dir: "{{ base_user_home }}"

spark_install_owner: "{{ base_install_owner }}"
spark_install_group: "{{ base_install_group }}"
spark_install_mode: "{{ base_install_mode }}"

spark_distribution_name: "netlib-hadoop{{ spark_hadoop_profile }}"
spark_distribution_basename: "spark-{{ spark_version }}-bin-{{ spark_distribution_name }}"
spark_distribution_url: "https://eta-hgi-bucket-hermes.cog.sanger.ac.uk/download/spark-{{ spark_version }}/{{ spark_distribution_basename }}.tgz"
spark_home: "{{ spark_install_dir }}/{{ spark_distribution_basename }}"
spark_pid_dir: "{{ spark_home }}/run"
spark_log_dir: "{{ spark_home }}/log"
spark_work_dir: "{{ spark_home }}/work"
spark_conf_dir: "{{ spark_home }}/conf"

spark_master_port: 7077

spark_env_sh:
  AWS_ACCESS_KEY_ID: "{{ aws_access_key_id | default('undef') }}"
  AWS_SECRET_ACCESS_KEY: "{{ aws_secret_access_key | default('undef') }}"

spark_local_dirs:
  - /mnt/tmpfs

spark_jars:
  - "{{ anaconda_install_dir }}/anaconda3/lib/python{{ anaconda_python_version }}/site-packages/hail/hail-all-spark.jar"
  - "{{ spark_install_dir }}/jars/*.jar"

spark_defaults_conf:
  # SparkContext Configuration
  spark.master: "spark://spark-master:{{ spark_master_port }}"
  spark.sql.files.maxPartitionBytes: "42949672960"
  spark.sql.files.openCostInBytes: "42949672960"

  spark.driver.host: spark-master
  spark.driver.bindAddress: 0.0.0.0
  spark.driver.memory: "{{ (((spark_master_memory | int()) / 1024) | int()) - 2 }}g"
  spark.driver.maxResultSize: 0

  spark.executor.memory: "{{ (((spark_slaves_memory | int()) / 1024) | int()) - 2 }}g"
  spark.executor.instances: 1

  spark.jars: "{{ spark_jars | join(',') }}"

  spark.serializer: "org.apache.spark.serializer.KryoSerializer"
  spark.kryo.registrator: "is.hail.kryo.HailKryoRegistrator"

  # Hadoop Configuration for S3
  spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
  spark.hadoop.fs.s3a.endpoint: "{{ aws_s3_endpoint | default('undef') }}"
  spark.hadoop.fs.s3a.access.key: "{{ aws_access_key_id | default('undef') }}"
  spark.hadoop.fs.s3a.secret.key: "{{ aws_secret_access_key | default('undef') }}"
  spark.hadoop.fs.s3a.connection.maximum: 100
  # spark.hadoop.fs.s3a.bucket.<bucket_name>.aws.credentials.provider:  org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider
  spark.ui.reverseProxy: true
  spark.ui.reverseProxyUrl: "http://{{ spark_master_external_address | default('localhost', true) }}/spark"


# Logger levels set to the official defaults
# Ref: https://github.com/apache/spark/blob/master/conf/log4j.properties.template
spark_log4j_extras:
  - name: log4j.logger.org.apache.spark.repl.Main
    value: WARN
  - name: log4j.logger.org.spark_project.jetty
    value: WARN
  - name: log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle
    value: ERROR
  - name: log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper
    value: INFO
  - name: log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
    value: INFO
  - name: log4j.logger.org.apache.parquet
    value: ERROR
  - name: log4j.logger.parquet
    value: ERROR
  - name: log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler
    value: FATAL
  - name: log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry
    value: ERROR

spark_sysctl_params:
  - name: kernel.msgmnb
    value: 65536
  - name: kernel.msgmax
    value: 65536
  - name: net.ipv4.tcp_max_tw_buckets
    value: 4000000
  - name: net.core.rmem_max
    value: 67108864
  - name: net.core.wmem_max
    value: 67108864
  - name: net.core.optmem_max
    value: 67108864
  - name: net.ipv4.tcp_rmem
    value: 4096 16777216 67108864
  - name: net.ipv4.tcp_wmem
    value: 4096 16777216 67108864
  - name: net.ipv4.tcp_mem
    value: 67108864 67108864 67108864
  - name: net.core.somaxconn
    value: 640000
  - name: net.core.netdev_max_backlog
    value: 250000
  - name: net.ipv4.tcp_max_syn_backlog
    value: 200000
  - name: net.ipv4.tcp_dsack
    value: 0
  - name: net.ipv4.tcp_sack
    value: 0
  - name: net.ipv4.tcp_window_scaling
    value: 1
  - name: net.ipv4.ip_local_port_range
    value: 8196 65535
  - name: net.ipv4.ip_local_reserved_ports
    # TODO: ensure spark_master_port spark_master_backend_port are on this list
    value: 7077,7337,8000-8088,8141,8188,8440-8485,8651-8670,8788,8983,9083,9898,10000-10033,10200,11000,13562,15000,19888,45454,50010,50020,50030,50060,50070,50075,50090,50091,50470,50475,50100,50105,50111,60010-60030
  - name: net.ipv4.tcp_retries2
    value: 10
  - name: net.ipv4.tcp_rfc1337
    value: 1
  - name: net.ipv4.tcp_fin_timeout
    value: 5
  - name: net.ipv4.tcp_keepalive_intvl
    value: 15
  - name: net.ipv4.tcp_keepalive_probes
    value: 5
  - name: vm.min_free_kbytes
    value: 204800
  - name: vm.page-cluster
    value: 20
  - name: vm.swappiness
    value: 10
  - name: fs.file-max
    value: 5049800

hail_install_dir: "{{ base_install_dir }}"
hail_install_owner: "{{ base_install_owner }}"
hail_install_group: "{{ base_install_group }}"
hail_install_mode: "{{ base_install_mode }}"

hail_home: "{{ hail_install_dir }}/hail"
hail_volume_keyfile: "{{ hail_install_dir }}/keyfile"
hail_volume_password: "{{ password }}"
hail_volume_device: /dev/vdb
hail_master_name: "hail-{{ deployment_owner | default('undef') }}.{{ domain_name | default('localdomain') }}"

jupyter_notebookapp_base_url: jupyter
jupyter_notebookapp_config:
  c.NotebookApp.allow_origin: "'*'"
  c.NotebookApp.base_url: "'{{ jupyter_notebookapp_base_url }}'"
  c.NotebookApp.ip: "'0.0.0.0'"
  c.NotebookApp.notebook_dir: "'{{ hail_install_dir }}/jupyter/data'"
  c.NotebookApp.open_browser: "False"
  c.NotebookApp.allow_password_change: "False"
  c.NotebookApp.quit_button: "False"

terraform_version: 0.11.14
terraform_sha256sum: 9b9a4492738c69077b079e595f5b2a9ef1bc4e8fb5596610f69a6f322a8af8dd

packer_version: 1.4.1
packer_sha256sum: b713ea79a6fb131e27d65ec3f2227f36932540e71820288c3c5ad770b565ecd7

provisioning_image_version: v0.5.1
provisioning_image_basename: "{{ datacenter }}-{{ programme }}-docker-provisioning-base-{{ provisioning_image_version }}.tar"
provisioning_image_url: "https://{{ datacenter }}-{{ programme }}-bucket-hermes.cog.sanger.ac.uk/download/containers/docker"
