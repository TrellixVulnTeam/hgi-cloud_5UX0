---
spark_version: 2.2.0
spark_mirror: https://archive.apache.org/dist/spark/

spark_user_name: "{{ base_user_name }}"
spark_group_name: "{{ base_group_name }}"

spark_source_dir: "{{ base_source_dir }}"
spark_download_dir: "{{ base_download_dir }}"
spark_install_dir: "{{ base_user_home }}"

spark_install_owner: "{{ base_install_owner }}"
spark_install_group: "{{ base_install_group }}"
spark_install_mode: "ug+rw,o+r"

spark_hadoop_profile: hadoop-2.7

spark_distribution_name: "hgi-hadoop{{ hadoop_version }}"
spark_home: "{{ spark_install_dir }}/spark-{{ spark_version }}-bin-{{ spark_distribution_name }}"

spark_local_owner: "{{ spark_user_name }}"
spark_local_group: "{{ spark_group_name }}"
spark_local_mode: "ug+rw,o+r,+t"
spark_local_dirs: []

spark_master_port: 7077
spark_master_host: spark-master

spark_env_sh: {}

spark_defaults_conf:
  # SparkContext Configuration
  spark.master: "spark://spark-master:{{ spark_master_port }}"
  spark.sql.files.maxPartitionBytes: "102400"
  spark.sql.files.openCostInBytes: "102400"

  spark.driver.memory: 1g
  spark.executor.memory: 1g

  # spark.jars: "{# spark_jars #}"
  # spark.ui.reverseProxy: true
  # spark.ui.reverseProxyUrl: "https://{# spark_master_external_hostname #}.{# spark_master_external_domain #}/{# spark_master_external_path #}"

  spark.serializer: "org.apache.spark.serializer.KryoSerializer"
  spark.kryo.registrator: "is.hail.kryo.HailKryoRegistrator"
  spark.local.dir: /tmp

  # {% if spark_executor_instances is defined %}
  # spark.executor.instances: "{# spark_executor_instances #}"
  # {% endif %}
  spark.driver.maxResultSize: 100

  # Extra classpaths (instead of using deprecated SPARK_CLASSPATH)
  spark.executor.extraClassPath: "{{ spark_executor_extra_classpath | join(':') }}"
  spark.driver.extraClassPath: "{{ spark_driver_extra_classpath | join(':') }}"

  # Hadoop Configuration for S3
  spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
  spark.hadoop.fs.s3a.endpoint: cog.sanger.ac.uk
  # {% for bucket in spark_hadoop_anonymous_buckets %}
  # spark.hadoop.fs.s3a.bucket.{# bucket #}.aws.credentials.provider  org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider
  # {% endfor %}
  spark.hadoop.fs.s3a.connection.maximum: 100

spark_pid_dir: "{{ spark_home }}/run"
spark_log_dir: "{{ spark_home }}/log"
spark_work_dir: "{{ spark_home }}/work"
spark_conf_dir: "{{ spark_home }}/conf"

# Logger levels set to the official defaults
# Ref: https://github.com/apache/spark/blob/master/conf/log4j.properties.template
spark_log4j_extras:
  - name: log4j.logger.org.apache.spark.repl.Main
    value: WARN
  - name: log4j.logger.org.spark_project.jetty
    value: WARN
  - name: log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle
    value: ERROR
  - name: log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper
    value: INFO
  - name: log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
    value: INFO
  - name: log4j.logger.org.apache.parquet
    value: ERROR
  - name: log4j.logger.parquet
    value: ERROR
  - name: log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler
    value: FATAL
  - name: log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry
    value: ERROR

spark_sysctl_params:
  - name: kernel.msgmnb
    value: 65536
  - name: kernel.msgmax
    value: 65536
  - name: net.ipv4.tcp_max_tw_buckets
    value: 4000000
  - name: net.core.rmem_max
    value: 67108864
  - name: net.core.wmem_max
    value: 67108864
  - name: net.core.optmem_max
    value: 67108864
  - name: net.ipv4.tcp_rmem
    value: 4096 16777216 67108864
  - name: net.ipv4.tcp_wmem
    value: 4096 16777216 67108864
  - name: net.ipv4.tcp_mem
    value: 67108864 67108864 67108864
  - name: net.core.somaxconn
    value: 640000
  - name: net.core.netdev_max_backlog
    value: 250000
  - name: net.ipv4.tcp_max_syn_backlog
    value: 200000
  - name: net.ipv4.tcp_dsack
    value: 0
  - name: net.ipv4.tcp_sack
    value: 0
  - name: net.ipv4.tcp_window_scaling
    value: 1
  - name: net.ipv4.ip_local_port_range
    value: 8196 65535
  - name: net.ipv4.ip_local_reserved_ports
    # TODO: ensure spark_master_port spark_master_backend_port are on this list
    value: 7077,7337,8000-8088,8141,8188,8440-8485,8651-8670,8788,8983,9083,9898,10000-10033,10200,11000,13562,15000,19888,45454,50010,50020,50030,50060,50070,50075,50090,50091,50470,50475,50100,50105,50111,60010-60030
  - name: net.ipv4.tcp_retries2
    value: 10
  - name: net.ipv4.tcp_rfc1337
    value: 1
  - name: net.ipv4.tcp_fin_timeout
    value: 5
  - name: net.ipv4.tcp_keepalive_intvl
    value: 15
  - name: net.ipv4.tcp_keepalive_probes
    value: 5
  - name: vm.min_free_kbytes
    value: 204800
  - name: vm.page-cluster
    value: 20
  - name: vm.swappiness
    value: 10
  - name: fs.file-max
    value: 5049800
