---
- name: Set spark_version fact
  set_fact:
    spark_version: "{{ spark_version_major }}.{{ spark_version_minor }}.{{ spark_version_patch }}"

- name: Set spark_home fact
  set_fact:
    spark_home: "{{ spark_install_dir }}/spark-{{ spark_version }}"

- name: Ensure Scala is installed
  apt:
    state: present
    name:
      - scala

- name: Create group account for Spark
  group:
    state: present
    name: "{{ spark_group_name }}"
    gid: "{{ spark_group_gid }}"
  tags: ["spark-group"]

- name: Create user account for Spark
  user:
    state: present
    name: "{{ spark_user_name }}"
    uid: "{{ spark_user_uid }}"
    group: "{{ spark_group_name }}"
    system: no
    home: "{{ spark_home }}"
    shell: "{{ spark_user_shell }}"
    groups: "{{ spark_user_groups | join(',') }}"
  tags: ["spark-user"]

- name: Ensure Spark download, source and install directories exist
  file:
    state: directory
    path: "{{ item }}"
    mode: 0755
    follow: true
  with_items:
    - "{{ spark_download_dir }}"
    - "{{ spark_install_dir }}"
    - "{{ spark_source_dir }}"

- name: Download Spark distribution
  get_url:
    url: "{{ spark_mirror }}/spark-{{ spark_version }}/spark-{{ spark_version }}.tgz"
    dest: "{{ spark_download_dir }}/spark-{{ spark_version }}.tgz"

# FIXME: This should really be implemented
#        https://www.apache.org/dyn/closer.lua/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz
#
# - name: Verify downloaded Spark distribution
#   command: /bin/true

- name: Extract Spark distribution
  become: false
  unarchive:
    src: "{{ spark_download_dir }}/spark-{{ spark_version }}.tgz"
    dest: "{{ spark_source_dir }}"
    copy: no
    # creates: "{{ spark_home }}"

- name: Install spark
  become: false
  shell: |
    JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \
    HADOOP_HOME=/opt/sanger.ac.uk/hadoop-2.9.2 \
    MAVEN_OPTS="-Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn" \
    ./dev/make-distribution.sh \
      --name hadoop2.9.2-netlib-lgpl \
      --tgz \
      -B \
      -Phadoop-2.9.2 \
      -Psparkr \
      -Pyarn \
      -DzincPort=3036 \
      -Pnetlib-lgpl \
      -Dhadoop.version=2.9.2
  args:
    chdir: "{{ spark_source_dir }}/spark-{{ spark_version }}"

# - name: Ensure Spark configuration directory exists
#   file:
#     state: directory
#     path: "{{ spark_home }}/conf"
#     owner: "{{ spark_user_name }}"
#     group: "{{ spark_group_name }}"
#   tags: ["config"]
# 
# - name: Ensure Spark log, run, temp and work directories exist
#   file:
#     state: directory
#     path: "{{ spark_home }}/{{ item }}"
#     owner: "{{ spark_user_name }}"
#     group: "{{ spark_group_name }}"
#     mode: 0775
#   with_items:
#     - log
#     - run
#     - temp
#     - work
# 
# - name: Ensure Spark local directories exist
#   file:
#     state: directory
#     path: "{{ item }}"
#     owner: "{{ spark_user_name }}"
#     group: "{{ spark_group_name }}"
#     mode: "{{ spark_local_dir_mode }}"
#   when: item != "/tmp"
#   with_items: "{{ spark_local_dirs }}"
#   tags: ["spark-local-dirs"]
# 
# - name: Configure Spark environment
#   template:
#     src: spark-env.sh.j2
#     dest: "{{ spark_home }}/conf/spark-env.sh"
#   tags: ["config"]
# 
# - name: Configure Spark defaults config file
#   template:
#     src: spark-defaults.conf.j2
#     dest: "{{ spark_home }}/conf/spark-defaults.conf"
#   tags: ["config"]
# 
# - name: Deploy Spark log4j properties
#   template:
#     src: log4j.properties.j2
#     dest: "{{ spark_home }}/conf/log4j.properties"
#     owner: "{{ spark_user_name }}"
#     group: "{{ spark_group_name }}"
# 
# - name: Create symlink to Spark versioned installation directory
#   file:
#     state: link
#     path: "{{ spark_install_dir }}/spark"
#     src: "{{ spark_home }}"
