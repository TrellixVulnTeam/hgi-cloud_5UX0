---
- name: Ensure Scala and R are installed
  apt:
    state: present
    name:
      - scala
      - r-base
      - r-recommended
      - libopenblas-base
    install_recommends: yes

- name: Ensure group account for Spark exists
  group:
    state: present
    name: "{{ spark_group_name }}"
#     gid: "{{ spark_group_gid }}"
#   tags: ["spark-group"]

- name: Ensure user account for Spark exists
  user:
    state: present
    name: "{{ spark_user_name }}"
#     uid: "{{ spark_user_uid }}"
#     group: "{{ spark_group_gid }}"
#     system: no
#     home: "{{ spark_home }}"
#     create_home: no
#     shell: "{{ spark_user_shell }}"
#     groups: "{{ spark_user_groups | join(',') }}"
#   tags: ["spark-user"]

- name: Ensure Spark download, source and install directories exist
  file:
    state: directory
    path: "{{ spark_build_dir }}"
    owner: "{{ spark_user_name }}"
    group: "{{ spark_group_name }}"
    mode: 0775
    follow: true
  loop:
    - "{{ spark_download_dir }}"
    - "{{ spark_install_dir }}"
    - "{{ spark_source_dir }}"
  loop_control:
    loop_var: spark_build_dir

- name: Download Spark distribution
  get_url:
    url: "{{ spark_mirror }}/spark-{{ spark_version }}/spark-{{ spark_version }}.tgz"
    dest: "{{ spark_download_dir }}/spark-{{ spark_version }}.tgz"

# TODO: This should really be implemented
#
# - name: Verify downloaded Spark distribution
#   command: /bin/true

- name: Extract Spark distribution
  unarchive:
    src: "{{ spark_download_dir }}/spark-{{ spark_version }}.tgz"
    dest: "{{ spark_source_dir }}"
    copy: no
    creates: "{{ spark_source_dir }}/spark-{{ spark_version }}/README.md"

- name: Build Spark
  shell: |
    R_HOME=/usr/lib/R \
    JAVA_HOME=/usr/lib/jvm/java-{{ java_jdk_version }}-openjdk-amd64 \
    MAVEN_OPTS="-Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn" \
    ./dev/make-distribution.sh \
        --name {{ spark_distribution_name }} \
        --tgz \
        -B \
        -Dhadoop.version={{ spark_hadoop_version }} \
        -Pnetlib-lgpl \
        -Psparkr \
        -Pyarn \
        -P{{ spark_hadoop_profile }} \
        -Phive \
        -Phive-thriftserver \
        -Pmesos
  args:
    chdir: "{{ spark_source_dir }}/spark-{{ spark_version }}"
    creates: "{{ spark_source_dir }}/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-{{ spark_distribution_name }}.tgz"

- name: Install Spark
  unarchive:
    src: "{{ spark_source_dir }}/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-{{ spark_distribution_name }}.tgz"
    dest: "{{ spark_install_dir }}"
    copy: no
    owner: "{{ spark_install_owner }}"
    group: "{{ spark_install_group }}"
    mode: "{{ spark_install_mode }}"
    creates: "{{ spark_install_dir }}/spark-{{ spark_version }}-bin-{{ spark_distribution_name }}/README.md"

- name: Ensure Spark runtime directories exist
  file:
    state: directory
    path: "{{ spark_runtime_dir }}"
    owner: "{{ spark_install_owner }}"
    group: "{{ spark_install_group }}"
    mode: "{{ spark_install_mode }}"
  loop:
    - "{{ spark_pid_dir }}"
    - "{{ spark_log_dir }}"
    - "{{ spark_temp_dir }}"
    - "{{ spark_work_dir }}"
    - "{{ spark_conf_dir }}"
  loop_control:
    loop_var: spark_runtime_dir

- name: Ensure Spark local directories exist
  file:
    state: directory
    path: "{{ local_dir }}"
    owner: "{{ spark_local_owner }}"
    group: "{{ spark_local_group }}"
    mode: "{{ spark_local_mode }}"
  when: local_dir != "/tmp"
  loop: "{{ spark_local_dirs }}"
  loop_control:
    loop_var: local_dir
  tags: ["spark-local-dirs"]

- name: Create symlink to Spark versioned installation directory
  file:
    state: link
    path: "{{ spark_install_dir }}/spark"
    src: "{{ spark_home }}"

- name: Configure system-wide environment variables
  template:
    src: profile.sh.j2
    dest: /etc/profile.d/spark.sh

- name: Configure Spark environment
  template:
    src: spark-env.sh.j2
    dest: "{{ spark_home }}/conf/spark-env.sh"
  tags: ["config"]

- name: Configure Spark defaults
  template:
    src: spark-defaults.conf.j2
    dest: "{{ spark_home }}/conf/spark-defaults.conf"
  tags: ["config"]

- name: Configure Spark log4j properties
  template:
    src: log4j.properties.j2
    dest: "{{ spark_home }}/conf/log4j.properties"
    owner: "{{ spark_user_name }}"
    group: "{{ spark_group_name }}"

- name: Tune sysctl parameters for Spark
  become: yes
  sysctl:
    state: present
    name: "{{ param.name }}"
    value: "{{ param.value }}"
    sysctl_file: /etc/sysctl.conf
  loop: "{{ spark_sysctl_params }}"
  loop_control:
    loop_var: param

- name: Install Spark services
  template:
    src: "spark.service.j2"
    dest: "/etc/systemd/system/spark-{{ service }}.service"
    owner: root
  become: yes
  loop:
    - master
    - slave
  loop_control:
    loop_var: service

- name: Ensure Spark services are disabled at this stage
  service:
    name: "spark-{{ service }}"
    enabled: no
    state: stopped
  become: yes
  loop:
    - master
    - slave
  loop_control:
    loop_var: service
