---
- name: Set spark_distribution_name fact
  set_fact:
    spark_distribution_name: "hgi-hadoop{{ spark_hadoop_version }}"

- name: Set spark_home fact
  set_fact:
    spark_home: "{{ spark_install_dir }}/spark-{{ spark_version }}-bin-{{ spark_distribution_name }}"

- name: Ensure Scala and R are installed
  apt:
    state: present
    name:
      - scala
      - r-base
      - r-recommended
      - libopenblas-base
    install_recommends: yes

- name: Create group account for Spark
  group:
    state: present
    name: "{{ spark_group_name }}"
    gid: "{{ spark_group_gid }}"
  tags: ["spark-group"]

- name: Create user account for Spark
  user:
    state: present
    name: "{{ spark_user_name }}"
    uid: "{{ spark_user_uid }}"
    group: "{{ spark_group_gid }}"
    system: no
    home: "{{ spark_home }}"
    create_home: no
    shell: "{{ spark_user_shell }}"
    groups: "{{ spark_user_groups | join(',') }}"
  tags: ["spark-user"]

- name: Ensure Spark download, source and install directories exist
  file:
    state: directory
    path: "{{ item }}"
    owner: "{{ spark_user_name }}"
    group: "{{ spark_group_name }}"
    mode: 0775
    follow: true
  with_items:
    - "{{ spark_download_dir }}"
    - "{{ spark_install_dir }}"
    - "{{ spark_source_dir }}"

- name: Download Spark distribution
  get_url:
    url: "{{ spark_mirror }}/spark-{{ spark_version }}/spark-{{ spark_version }}.tgz"
    dest: "{{ spark_download_dir }}/spark-{{ spark_version }}.tgz"

# TODO: This should really be implemented
#
# - name: Verify downloaded Spark distribution
#   command: /bin/true

- name: Extract Spark distribution
  unarchive:
    src: "{{ spark_download_dir }}/spark-{{ spark_version }}.tgz"
    dest: "{{ spark_source_dir }}"
    copy: no
    owner: "{{ spark_user_name }}"
    group: "{{ spark_group_name }}"
    mode: "u+rw,go+r"
    creates: "{{ spark_source_dir }}/spark-{{ spark_version }}"

- name: Build Spark
  shell: |
    R_HOME=/usr/lib/R \
    JAVA_HOME=/usr/lib/jvm/java-{{ java_jdk_version }}-openjdk-amd64 \
    MAVEN_OPTS="-Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn" \
    ./dev/make-distribution.sh \
        --name {{ spark_distribution_name }} \
        --tgz \
        -B \
        -Dhadoop.version={{ spark_hadoop_version }} \
        -Pnetlib-lgpl \
        -Psparkr \
        -Pyarn \
        -P{{ spark_hadoop_profile }} \
        -Phive \
        -Phive-thriftserver \
        -Pmesos
  args:
    chdir: "{{ spark_source_dir }}/spark-{{ spark_version }}"
    creates: "{{ spark_source_dir }}/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-{{ spark_distribution_name }}.tgz"

- name: Install Spark
  unarchive:
    src: "{{ spark_source_dir }}/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-{{ spark_distribution_name }}.tgz"
    dest: "{{ spark_install_dir }}"
    copy: no
    owner: "{{ spark_install_owner }}"
    group: "{{ spark_install_group }}"
    mode: "u+rw,go+r"
    creates: "{{ spark_install_dir }}/spark-{{ spark_version }}-bin-{{ spark_distribution_name }}"

- name: Ensure Spark conf, log, run, temp and work directories exist
  file:
    state: directory
    path: "{{ spark_home }}/{{ spark_dir }}"
    owner: "{{ spark_install_owner }}"
    group: "{{ spark_install_group }}"
    mode: 0775
  loop:
    - log
    - run
    - temp
    - work
    - conf
  loop_control:
    loop_var: spark_dir

- name: Ensure Spark local directories exist
  file:
    state: directory
    path: "{{ local_dir }}"
    owner: "{{ spark_local_owner }}"
    group: "{{ spark_local_group }}"
    mode: "{{ spark_local_mode }}"
  when: local_dir != "/tmp"
  loop: "{{ spark_local_dirs }}"
  loop_control:
    loop_var: local_dir
  tags: ["spark-local-dirs"]

- name: Create symlink to Spark versioned installation directory
  file:
    state: link
    path: "{{ spark_install_dir }}/spark"
    src: "{{ spark_home }}"

# - name: Configure Spark environment
#   template:
#     src: spark-env.sh.j2
#     dest: "{{ spark_home }}/conf/spark-env.sh"
#   tags: ["config"]
# 
# - name: Configure Spark defaults config file
#   template:
#     src: spark-defaults.conf.j2
#     dest: "{{ spark_home }}/conf/spark-defaults.conf"
#   tags: ["config"]
# 
# - name: Deploy Spark log4j properties
#   template:
#     src: log4j.properties.j2
#     dest: "{{ spark_home }}/conf/log4j.properties"
#     owner: "{{ spark_user_name }}"
#     group: "{{ spark_group_name }}"
#
# - name: Tune sysctl parameters for spark
#   become: yes
#   sysctl:
#     state: present
#     name: "{{ param.name }}"
#     value: "{{ param.value }}"
#     sysctl_file: /etc/sysctl.conf
#   loop: "{{ spark_sysctl_params }}"
#   loop_control:
#     loop_var: param
