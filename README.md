# hgi-systems-cluster-spark

A reboot of the HGI's IaC project. This specific project has been created to
address one, simple, initial objective: the lifecycle management of a spark cluster.

# Why a reboot?

The code was not effective any more: the team was not confident with the
codebase, the building process and the infrastructure generated by the code
was missing a number of must-have features for today's infrastructures.
We chose to have a fresh start on the IaC, rather then refactoring legacy
code. This will let us choose simple and effective objectives, outline better
requirements, and design around operability from the very beginning.

# Architecture
TODO: include a simple design diagram

# Design choices

## The common environment
Regardless of the Openstack's project, all environments will require some
common resources such as `keypair`s or `secgroup`s, therefore a module for the
`common` environment has been created. Before you create/mange any resource,
you should ensure that the all the resources in [the common
environment](terraform/modules/openstack/environments/common) are created. Read
the [Usage](#usage) section below.

# Use cases

## Create a Spark Cluster
TODO: wirte down the use case's details

## Destroy a Spark Cluster
TODO: wirte down the use case's details

# Usage

## What do you need?
1. (required) `terraform` executable anywhere in your `PATH`
2. (required) `packer` executable anywhere in your `PATH`
4. (optional) `openrc.sh` OpenStack's RC file, that you can get from
   OpenStack's web interface. Alternatively, you could manually export the
   shell environment variables required to configure the OpenStack provider in
   `terraform`.

### invoke.sh
`invoke.sh` is shell script made to wrap `pyinvoke` quite extensive list of
tasks and collections, and meke its usage even easier. `invoke.sh` will fulfill
all the [requirements for invoke](invoke/README.md#what-do-you-need): it will
create the `python3` virtual environment named `py3` with all the required
`python3` modules, read all the bash environment variables in `openrc.sh` and
then run `invoke` with the appropriate options and tasks. To understand how to
use `invoke.sh`, you can run:
```
bash invoke.sh --help
```
`invoke.sh` is quite straightforward and you are very wellcome to read the
source code.

## The easy way
The easy way to use this project is through the [invoke.sh](#invoke-sh)
shellscript from the base directory of this repository.

To build the base Openstack image:
```
bash invoke.sh base_image build
```

To create an environment:
```
bash invoke.sh common_environment up
```
The above command will run `terraform` with the correct variables file, and
against the correct initial module. It also depends on other tasks:
1. `clean`
2. `init`
3. `validate`
4. `plan`
which will automatically run in the correct order. You can replace
`common_environment` with other values. 
To get the list of available tasks collections, you can run:
```
bash invoke.sh --list
```

To update an environment:
```
bash invoke.sh dev_environment plan --to update
# At this point you should check the update.tfplan
bash invoke.sh dev_environment update
```
The `update` task won't run unless there is an `update.tfplan` file available,
but that file is not automatically created as a result of running other
`invoke`'s tasks: the rationale is that we don't want you to unwillingly modify
the infrastructure. That being said, the `up` task is effectively the same as:
```
bash invoke.sh dev_environment plan --to update
bash invoke.sh dev_environment update
```
This means that you could use the `up` task to bypass the "2 steps" approach,
**if you know what you are doing**. The `up` tasks just uses different file
names and options which let all subtasks to run automatically.

To destroy an environment:
```
bash invoke.sh spark_environment plan --to destroy
# At this point you should check the destroy.tfplan
bash invoke.sh spark_environment down
```
The `down` task won't run unless there is a `destroy.tfplan` file available.
The same rationale for to the `update` task is applied here, however, there is
no task that bypasses the 2 steps. If you really (I mean REALLY) know what you
are doing, you could run 2 tasks with the same `invoke.sh` run:
```
bash invoke.sh spark_environment plan --to destroy down
```

## The hard way
If you know your way around `OpenStack`, `packer` and `terraform` and you know
how to setup the bash environment variables you need, then you are wellcome to
run the appropriate terraform/packer commands to get your tasks done. Feel free
to take inspiration from `invoke.sh` and all the `tasks` and `collection` files
you can find in the `invoke/` directory.

# How to contribute
Read the [CONTRIBUTING.md](CONTRIBUTING.md) file

# Licese
Read the [LICENSE.md](LICENSE.md) file
